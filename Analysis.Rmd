---
title: "Predicting the quality of weight lifting exercise activity"
output: html_document
---

## The analysis is divided into two main sections
### First: Exploratory data analysis

Load libraries and data

```{r}
library(dplyr)
library(magrittr)
library(caret)
library(doParallel)

Training <- read.csv("/Users/amrmostafa/Documents/Data Science/Course8/Project/pml-training.csv",stringsAsFactors = FALSE)
Testing <- read.csv("/Users/amrmostafa/Documents/Data Science/Course8/Project/pml-testing.csv",stringsAsFactors = FALSE)
```

####Check the training set

```{r}
dim(Training)
str(Training)
summary(Training)

Tab <- table(Training$classe)
Tab
prop.table(Tab)

Training$new_window[1:50]
sum(Training$new_window == "yes")
length(unique(Training$num_window))

Training %>% select(1:10, max_roll_belt, avg_roll_arm) %>% head(60)
Training %>% select(1:10, max_roll_belt, avg_roll_arm) %>% tail(50)
Training %>% select(1:10, max_roll_belt, avg_roll_arm) %>% head(60)
```

Notes:
The new_window variable suggests that there are 406 windows in total while checking the variable reveals 858 different window

####Check the testing set
```{r}
dim(Testing)
Testing[1:10, 1:13]

checkpoint <- sapply (Testing, function(x)all(is.na(x)))
sum(checkpoint)

column_name <- names(checkpoint[checkpoint == FALSE])
column_name

Training_window_num <- unique(Training$num_window)
Testing_window_num <- unique(Testing$num_window)

which(Testing_window_num %in% Training_window_num)

```

####Conculsion:
->We should predict the classes of 20 singl observations in the testing set
->We can't try different window sizes since we have to deal with 20 single test cases
->100 out of 160 columns are NA in the testing set and accordingly we should consider the columns for building the model based on the training set


More investigation will be applied below:

```{r}
Training_window_num <- unique(Training$num_window)
Testing_window_num <- unique(Testing$num_window)

which(Testing_window_num %in% Training_window_num)

Testing[1, 1:8]
Training %>% filter(num_window == 74, raw_timestamp_part_1 == 1323095002) %>% select(1:8, classe)

Testing[2, 1:8]
Training %>% filter(num_window == 431, raw_timestamp_part_1 == 1322673067) %>% select(1:8, classe)

Testing[3, 1:8]
Training %>% filter(num_window == 439, raw_timestamp_part_1 == 1322673075) %>% select(1:8, classe)


```

####Conculsion:
->20 observations from the testing set are simply cut out from the training set which shown at observations in the training set and match the num_window variable of one of the testing set observations

->Accordingly, we can build a look up fun. instead of creating a prediction model

Let's try our solution here:

```{r}
predictions <- rep(NA,20)
for(i in seq_along(Testing_window_num)) {
  predictions[i] <- Training %>%
    filter(num_window == Testing_window_num[i]) %>%
    select(classe) %>%
    slice(1) %>% unlist
}
##output
predictions
```

####Note
predictions vector will be used as input for the pml_write_files function as shown below

```{r setup, include=TRUE}


pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("/Users/amrmostafa/Documents/Data Science/Course8/Project/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
##output
pml_write_files(predictions)

```

###Results
-> The submitted predictions files based on the look-up function's results worked properly
-> All testing set observation were predicated correctly

### Second: Train and build a random forest model

Since we need to report an estimate for out of sample error based on cross validation, we will train and build a random forest model

```{r}
sub_Training <- Training %>%
  filter(new_window == "yes")

sub_Training %<>% select(-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window)) %>%
  mutate(
    classe = as.factor(classe)
  )


sub_Training[sub_Training == "#DIV/0!"] <- NA

comp_na_columns <- sapply(sub_Training, function(x) any(is.na(x))) %>%
  .[. == TRUE] %>%
  names

sub_Training %<>% select(-one_of(comp_na_columns))
```
Detect Cores=
```{r}
detectCores()
```
Get do Par Workers=
```{r}
getDoParWorkers()
```


```{r}
registerDoParallel(cores = 4)
```

#####Define resampling schema
```{r}
ctrl <- trainControl(method = 'cv', number = 10)
```

#####Train random forest model
```{r}
grid <-expand.grid(mtry = seq(2, ncol(sub_Training), length.out = 5))

rf_fit <- train(classe ~ ., data = sub_Training, method = "rf", tuneGrid=grid, ntree = 1000, trControl = ctrl)
```

#####Calculate in sample error
```{r}
confusionMatrix(predict(rf_fit, newdata = sub_Training), sub_Training$classe)
```

####Sample error
The sample error is 0% because all observations are classified correctly

###Calculate out of sample error
```{r}

rf_fit
```

#####Note
Using 10-fold cross validatoin we achieved the best accuarcy which means that out of sample error is 16%

```{r}
rf_fit$finalModel
```
